{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPoGrZb9kVXOXYfDt0GxCB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aya11ali/Shouf/blob/main/Comment_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comment Analysis"
      ],
      "metadata": {
        "id": "QS6MPK5jYRU7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ka_mevGXYGIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8105c62-5716-4a3a-cfcc-be127331c9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q transformers torch langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from langdetect import detect, LangDetectException\n",
        "import re\n",
        "from abc import ABC, abstractmethod"
      ],
      "metadata": {
        "id": "v86xdqQ8ZLbr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IModelLoader(ABC):\n",
        "  @abstractmethod\n",
        "  def load_model(self):\n",
        "    pass\n",
        "\n",
        "class English_model(IModelLoader):\n",
        "  def __init__(self, toxicity_model=\"unitary/toxic-bert\"):\n",
        "    self.toxicity_model = toxicity_model\n",
        "    self.sentiment_analyzer = None\n",
        "    self.toxic_analyzer = None\n",
        "\n",
        "  def load_model(self):\n",
        "      # English models\n",
        "      self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "      self.toxic_analyzer = pipeline(\"text-classification\", model=self.toxicity_model)\n",
        "\n",
        "class Arabic_model(IModelLoader):\n",
        "  def __init__(self,sentiment_model=\"CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment\",\n",
        "               toxic_model=\"Hate-speech-CNERG/dehatebert-mono-arabic\"):\n",
        "    self.sentiment_model = sentiment_model\n",
        "    self.toxic_model = toxic_model\n",
        "    self.sentiment_analyzer = None\n",
        "    self.toxic_analyzer = None\n",
        "\n",
        "  def load_model(self):\n",
        "      # Arabic models\n",
        "      self.sentiment_analyzer = pipeline(\"text-classification\", model=self.sentiment_model)\n",
        "      self.toxic_analyzer = pipeline(\"text-classification\", model=self.toxic_model)\n"
      ],
      "metadata": {
        "id": "fPSo_t5bYcWp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    def __init__(self,toxicity_threshold=0.7):\n",
        "      self.toxicity_threshold = toxicity_threshold"
      ],
      "metadata": {
        "id": "XdLv-V6teJGT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interface\n",
        "class ITranslator(ABC):\n",
        "    @abstractmethod\n",
        "    def load(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def translate(self, text: str) -> str:\n",
        "        pass\n",
        "\n",
        "class Translator(ITranslator):\n",
        "    def __init__(self,model_name='Helsinki-NLP/opus-mt-ar-en'):\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load(self):\n",
        "        self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = MarianMTModel.from_pretrained(self.model_name)\n",
        "\n",
        "    def translate(self, text: str) -> str:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "        translated = self.model.generate(**inputs)\n",
        "        return self.tokenizer.decode(translated[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "0VjlR4w_bfnG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Iclean(ABC):\n",
        "    @abstractmethod\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        pass\n",
        "\n",
        "class Clean(Iclean):\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"http\\S+\", \"\", text)  # remove URLs\n",
        "        text = re.sub(r\"@\\w+\", \"\", text)     # remove mentions\n",
        "        text = re.sub(r\"#[A-Za-z0-9_]+\", \"\", text)  # remove hashtags\n",
        "        text = re.sub(r\"[^a-zA-Zأ-ي\\s]\", \"\", text)  # remove symbols except Arabic\n",
        "        return text.strip()"
      ],
      "metadata": {
        "id": "Z23uQnSzfv-7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ILanguageDetector(ABC):\n",
        "  @abstractmethod\n",
        "  def detect_language(self, text: str) -> str:\n",
        "    pass\n",
        "\n",
        "class LanguageDetector(ILanguageDetector):\n",
        "  def detect_language(self, text: str) -> str:\n",
        "    try:\n",
        "      return detect(text)\n",
        "    except LangDetectException:\n",
        "      return None"
      ],
      "metadata": {
        "id": "NFKDmYeIqeCJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IAnalyzer(ABC):\n",
        "    @abstractmethod\n",
        "    def analyze(self, text: str) -> dict:\n",
        "        pass\n",
        "\n",
        "class CommentAnalyzer(IAnalyzer):\n",
        "    def __init__(self, model_loader: IModelLoader, translator: ITranslator = None,config:ModelConfig = None):\n",
        "        self.model_loader = model_loader\n",
        "        self.translator = translator\n",
        "        self.config=config\n",
        "        self.model_loader.load_model()\n",
        "\n",
        "    def analyze(self, text: str) -> dict:\n",
        "        # self.model_loader.load_model()\n",
        "        temp_text = text\n",
        "        if self.translator:\n",
        "            self.translator.load()\n",
        "            temp_text = self.translator.translate(temp_text)\n",
        "        sentiment = self.model_loader.sentiment_analyzer(temp_text)[0]\n",
        "        toxicity = self.model_loader.toxic_analyzer(temp_text)[0]\n",
        "\n",
        "        toxicity_score = round(toxicity[\"score\"], 3)\n",
        "        toxicity_label = \"TOXIC\" if toxicity_score > self.config.toxicity_threshold else \"NON_TOXIC\"\n",
        "\n",
        "        return {\n",
        "            \"comment\" : text,\n",
        "            \"sentiment\": sentiment[\"label\"],\n",
        "            \"toxicity_score\": toxicity_score,\n",
        "            \"toxicity_label\": toxicity_label\n",
        "        }"
      ],
      "metadata": {
        "id": "fm60_LTds4PS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IBatch(ABC):\n",
        "  @abstractmethod\n",
        "  def comments_batch(self,comments:list)->list:\n",
        "    pass\n",
        "\n",
        "class Batch(IBatch):\n",
        "  def __init__(self, analyzer: IAnalyzer):\n",
        "    self.analyzer = analyzer\n",
        "\n",
        "  def comments_batch(self,comments:list)->list:\n",
        "    return [self.analyzer.analyze(comment) for comment in comments]\n"
      ],
      "metadata": {
        "id": "gUScyu2a0h68"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CommentProcessingPipeline:\n",
        "  def __init__ (self, config=ModelConfig(toxicity_threshold=0.7)):\n",
        "    self.config = config\n",
        "    self.language_detector = LanguageDetector()\n",
        "    self.cleaner = Clean()\n",
        "\n",
        "    self.english_model = English_model()\n",
        "    self.arabic_model = Arabic_model()\n",
        "    self.translator = Translator()\n",
        "\n",
        "    self.english_analyzer = CommentAnalyzer(self.english_model,None,self.config)\n",
        "    self.arabic_analyzer = CommentAnalyzer(self.english_model, self.translator,self.config)\n",
        "    # self.batch = Batch()\n",
        "\n",
        "  # def load(self):\n",
        "  #   self.translator.load()\n",
        "\n",
        "  def process_comment(self, comment: str) -> dict:\n",
        "    cleaned = self.cleaner.clean_text(comment)\n",
        "    lang = self.language_detector.detect_language(cleaned)\n",
        "\n",
        "    if lang == \"ar\":\n",
        "        analyzer = self.arabic_analyzer\n",
        "    elif lang == \"en\":\n",
        "        analyzer = self.english_analyzer\n",
        "    else:\n",
        "        return {\"error\": \"Unsupported or undetectable language\"}\n",
        "\n",
        "    return analyzer.analyze(cleaned)\n",
        "\n",
        "  def process_comments(self, comments: list) -> list:\n",
        "      return [self.process_comment(comment) for comment in comments]\n",
        "\n"
      ],
      "metadata": {
        "id": "TCnAoHz33R09"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Functions\n"
      ],
      "metadata": {
        "id": "HxAeLRdzrOCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import logging\n",
        "logging.set_verbosity_error()  # Only errors will be shown\n"
      ],
      "metadata": {
        "id": "ACPwDvkvYuWN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    comment_pipeline = CommentProcessingPipeline()\n",
        "\n",
        "    comments = [\n",
        "        \"This is terrible!\",\n",
        "        \"هذا تعليق سام\",\n",
        "        \"I love this product!\",\n",
        "        \"منتج فظيع\",\n",
        "        \"أنت غبي وما تفهم أي شيء، مكانك في الزبالة\"\n",
        "    ]\n",
        "\n",
        "    results = comment_pipeline.process_comments(comments)\n",
        "    for res in results:\n",
        "        print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql-9uj9UWqhP",
        "outputId": "2a78af54-8812-4e5a-f28e-c023ae5c1d79"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'comment': 'this is terrible', 'sentiment': 'NEGATIVE', 'toxicity_score': 0.062, 'toxicity_label': 'NON_TOXIC'}\n",
            "{'comment': 'هذا تعليق سام', 'sentiment': 'POSITIVE', 'toxicity_score': 0.001, 'toxicity_label': 'NON_TOXIC'}\n",
            "{'comment': 'i love this product', 'sentiment': 'POSITIVE', 'toxicity_score': 0.001, 'toxicity_label': 'NON_TOXIC'}\n",
            "{'comment': 'منتج فظيع', 'sentiment': 'NEGATIVE', 'toxicity_score': 0.021, 'toxicity_label': 'NON_TOXIC'}\n",
            "{'comment': 'أنت غبي وما تفهم أي شي مكانك في الزبالة', 'sentiment': 'NEGATIVE', 'toxicity_score': 0.987, 'toxicity_label': 'TOXIC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srmoucCzYRHw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}