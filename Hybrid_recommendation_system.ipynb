{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS9aEsXTFLgkH3k5ETDMTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aya11ali/Shouf/blob/main/Hybrid_recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install camel-tools"
      ],
      "metadata": {
        "id": "TJgjAlj69NaS"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.23.5"
      ],
      "metadata": {
        "id": "nJh7trDm1RMK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q gensim"
      ],
      "metadata": {
        "id": "Dwgmw4ka9XG-"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0dWFpbczaUY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Iterable, Dict, Union,Tuple\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse.linalg import svds\n",
        "\n",
        "import os\n",
        "import scipy\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import torch\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "# from camel_tools.tokenizers.word import simple_word_tokenize as arabic_tokenizer\n",
        "# from camel_tools.morphology.analyzer import Analyzer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import joblib\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "jG8I1LhnCUd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e780777-a256-404e-f8aa-c4da5d3d2c50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collaborative Filtering Recommendation System**"
      ],
      "metadata": {
        "id": "jqKvslPxGn3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ILoad_and_shuffle_Dataset(ABC):\n",
        "  @abstractmethod\n",
        "  def load_dataset(self):\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def shuffle_data(self):\n",
        "    pass\n",
        "\n",
        "class Load_and_shuffle_Dataset(ILoad_and_shuffle_Dataset):\n",
        "  def __init__(self,dataset_path:str,random_seed:int=42):\n",
        "    self.random_seed = random_seed\n",
        "    np.random.seed(self.random_seed) # Set seed for NumPy's random number generator to ensure reproducible results\n",
        "    self.dataset_path = dataset_path\n",
        "    self.df = None\n",
        "    self.load_dataset()\n",
        "    self.shuffle_data()\n",
        "\n",
        "\n",
        "  def load_dataset(self):\n",
        "    self.df = pd.read_csv(self.dataset_path)\n",
        "\n",
        "  def shuffle_data(self):\n",
        "    self.df = self.df.sample(frac=1, random_state=self.random_seed).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FY6hQMCPkdLf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following module defines a structure for calculating interaction scores in a dataset based on different user actions (e.g., \"watch\", \"like\", \"comment\", \"subscribe\"). It uses a column-matching strategy to identify relevant columns, applies weights to interaction types, and computes a final score for each record."
      ],
      "metadata": {
        "id": "dtc8PjdoKCyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IInteractionScorer(ABC):\n",
        "    @abstractmethod\n",
        "    def compute_interaction_score(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ColumnMatcher(ABC):\n",
        "    @abstractmethod\n",
        "    def match_columns(self, df_columns, interaction_type):\n",
        "        pass\n",
        "\n",
        "\n",
        "class SubstringMatcher(ColumnMatcher):\n",
        "    def match_columns(self, df_columns, interaction_type=None):\n",
        "        if interaction_type is None:\n",
        "            raise ValueError(\"interaction_type must be provided\")\n",
        "        return [col for col in df_columns if interaction_type in col]\n",
        "\n",
        "\n",
        "class InteractionScorer(IInteractionScorer):\n",
        "    def __init__(self, dataframe, weights:dict=None, matcher: ColumnMatcher = SubstringMatcher):\n",
        "        self.df = dataframe\n",
        "        self.weights = weights or {\n",
        "            \"watch\": 1,\n",
        "            \"like\": 2,\n",
        "            \"comment\": 3,\n",
        "            \"subscribe\": 5\n",
        "        }\n",
        "        self.matcher = matcher()\n",
        "        self.compute_interaction_score()\n",
        "\n",
        "    def compute_interaction_score(self):\n",
        "        interaction_score = 0\n",
        "        for interaction_type, weight in self.weights.items():\n",
        "            matched_cols = self.matcher.match_columns(self.df.columns, interaction_type)\n",
        "            for col in matched_cols:\n",
        "                interaction_score += self.df[col] * weight\n",
        "        self.df[\"interaction_score\"] = interaction_score\n"
      ],
      "metadata": {
        "id": "GF2wDkvrn2ne"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreparingManager:\n",
        "  def __init__(self, loader:ILoad_and_shuffle_Dataset, dataset_path:str,score:IInteractionScorer):\n",
        "    self.loader = loader(dataset_path)\n",
        "    self.df = self.loader.df\n",
        "    self.interaction_score = score(self.df)\n",
        "\n",
        "  def prepare_data(self):\n",
        "    return self.df"
      ],
      "metadata": {
        "id": "LolC3gbm24Qr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ISparseMatrixConverter(ABC):\n",
        "    @abstractmethod\n",
        "    def create_sparse_matrix(self)  -> csr_matrix:\n",
        "        pass\n",
        "\n",
        "class SparseMatrixConverter(ISparseMatrixConverter):\n",
        "    def __init__(self, dense_matrix: pd.DataFrame):\n",
        "        self.df = dense_matrix\n",
        "        self._sparse_matrix = None\n",
        "\n",
        "    def create_sparse_matrix(self)  -> csr_matrix:\n",
        "        if self._sparse_matrix is None:\n",
        "            self._sparse_matrix = csr_matrix(self.df.values)\n",
        "        return self._sparse_matrix\n",
        "\n",
        "    @property\n",
        "    def sparse_matrix(self):\n",
        "        return self.create_sparse_matrix()"
      ],
      "metadata": {
        "id": "juG4qMy2Yz8v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IKNNModelTrainer(ABC):\n",
        "    @abstractmethod\n",
        "    def train_model(self) -> NearestNeighbors:\n",
        "        pass\n",
        "\n",
        "class KNNModelTrainer(IKNNModelTrainer):\n",
        "    def __init__(self, sparse_matrix: csr_matrix, metric: str = \"cosine\",\n",
        "                 algorithm: str = \"brute\"):\n",
        "        self.sparse_matrix = sparse_matrix\n",
        "        self.metric = metric\n",
        "        self.algorithm = algorithm\n",
        "        self._model = None\n",
        "\n",
        "    def train_model(self) -> NearestNeighbors:\n",
        "      try:\n",
        "          if self._model is None:\n",
        "              self._model = NearestNeighbors(metric=self.metric,\n",
        "                                             algorithm=self.algorithm)\n",
        "              self._model.fit(self.sparse_matrix)\n",
        "\n",
        "      except Exception as e:\n",
        "          raise TrainingError(f\"Model training failed: {e}\")\n",
        "\n",
        "      return self._model\n",
        "\n",
        "    @property\n",
        "    def model(self) -> NearestNeighbors:\n",
        "        return self.train_model()\n"
      ],
      "metadata": {
        "id": "AVWiaNr1crGD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Based Filter"
      ],
      "metadata": {
        "id": "zIf414ti-ehZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following  module defines an abstract interface and a concrete implementation for creating a ***User-Item interaction*** matrix from a given dataset.\n",
        "\n",
        "`UserItemMatrix` (Concrete Implementation)\n",
        "\n",
        "\n",
        "Implements `IUserItemMatrix` to generate a matrix from a DataFrame using specified rows, columns, and values.\n",
        "\n",
        "**Constructor Parameters:**\n",
        "\n",
        "`dataframe` (pd.DataFrame): Input dataset.\n",
        "\n",
        "`row` (str): Column to use as index (e.g., user_id).\n",
        "\n",
        "`column` (str): Column to use as columns (e.g., video_id).\n",
        "\n",
        "`values` (str): Column containing interaction values (e.g., interaction_score)."
      ],
      "metadata": {
        "id": "LxkX1pOiYAqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IUserItemMatrix(ABC):\n",
        "  @abstractmethod\n",
        "  def create_user_item_matrix(self)->pd.DataFrame:\n",
        "    pass\n",
        "\n",
        "class UserItemMatrix(IUserItemMatrix):\n",
        "  def __init__(self, dataframe:pd.DataFrame, row:str, column:str, values:str):\n",
        "    self.df = dataframe\n",
        "    self.row = row\n",
        "    self.column = column\n",
        "    self.values = values\n",
        "\n",
        "  def create_user_item_matrix(self)->pd.DataFrame:\n",
        "    if self.row not in self.df.columns or self.column not in self.df.columns:\n",
        "        raise InvalidMatrixError(f\"Invalid columns: {self.row} or {self.column} not found in the dataframe.\")\n",
        "\n",
        "    return self.df.pivot(index=self.row, columns=self.column, values=self.values).fillna(0)\n",
        "\n",
        "  @property\n",
        "  def matrix(self) -> pd.DataFrame:\n",
        "      return self.create_user_item_matrix()"
      ],
      "metadata": {
        "id": "j4UZ4MWhG2pO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class Validates whether a given user_id exists in the user-item interaction matrix, ensuring the user has interacted with the system and that the matrix is properly initialized."
      ],
      "metadata": {
        "id": "OLGlxo5NnftY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IUserInteractionChecker(ABC):\n",
        "    @abstractmethod\n",
        "    def check_user_interaction(self, user_id: int, user_item_matrix: pd.DataFrame) -> bool:\n",
        "        pass\n",
        "\n",
        "class UserInteractionChecker(IUserInteractionChecker):\n",
        "    def check_user_interaction(self, user_id: int, user_item_matrix: pd.DataFrame) -> bool:\n",
        "        if user_item_matrix is None or user_item_matrix.empty:\n",
        "            raise InvalidMatrixError(\"User-Item matrix is empty or not initialized.\")\n",
        "        if user_id not in user_item_matrix.index:\n",
        "            raise InvalidUserIdError(user_id)\n",
        "        return True\n"
      ],
      "metadata": {
        "id": "KAR93o8YNpwr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IUserIndexFinder(ABC):\n",
        "    @abstractmethod\n",
        "    def _find_user_index(self) -> int:\n",
        "        \"\"\"Finds the index of the user in the user-item matrix.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def index(self):\n",
        "        pass\n",
        "\n",
        "class UserIndexFinder(IUserIndexFinder):\n",
        "    def __init__(self, user_id: int, user_item_matrix: pd.DataFrame, user_check: IUserInteractionChecker):\n",
        "        self.user_id = user_id\n",
        "        self.user_item_matrix = user_item_matrix\n",
        "        self.user_check = user_check\n",
        "\n",
        "    def _find_user_index(self) -> int:\n",
        "        # Check user interaction when needed\n",
        "        self.user_check.check_user_interaction(self.user_id, self.user_item_matrix)\n",
        "\n",
        "        return self.user_item_matrix.index.get_loc(self.user_id)\n",
        "\n",
        "    @property\n",
        "    def index(self) -> int:\n",
        "        return self._find_user_index()\n"
      ],
      "metadata": {
        "id": "pO2an0dgQCtI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ISimilarUserFinder(ABC):\n",
        "  @abstractmethod\n",
        "  def find_similar_users(self, n_neighbors: int = 5) -> np.ndarray:\n",
        "      \"\"\"Returns indices of similar users for a given user ID.\"\"\"\n",
        "      pass\n",
        "\n",
        "  @property\n",
        "  @abstractmethod\n",
        "  def similar_users(self) -> np.ndarray:\n",
        "      \"\"\"Returns cached similar users.\"\"\"\n",
        "      pass\n",
        "\n",
        "class SimilarUserFinder(ISimilarUserFinder):\n",
        "  def __init__(self, user_id: int, knn_model, user_item_matrix: pd.DataFrame,\n",
        "              user_index: IUserIndexFinder, user_check: IUserInteractionChecker):\n",
        "      self.user_id = user_id\n",
        "      self.knn_model = knn_model\n",
        "      self.user_item_matrix = user_item_matrix\n",
        "      self.user_check = user_check\n",
        "      self.user_index = user_index\n",
        "      self._similar_users = None\n",
        "\n",
        "      self.user_check.check_user_interaction(self.user_id, self.user_item_matrix)\n",
        "      self._user_index = self.user_index.index\n",
        "\n",
        "  def find_similar_users(self, n_neighbors: int = 5) -> np.ndarray:\n",
        "\n",
        "      if self.knn_model is None:\n",
        "          raise RecommendationSystemError(\"KNN model is not provided.\")\n",
        "\n",
        "      # Get the user vector\n",
        "      user_vector = self.user_item_matrix.iloc[self._user_index].values.reshape(1, -1)\n",
        "\n",
        "      # Find neighbors\n",
        "      distances, indices = self.knn_model.kneighbors(user_vector,\n",
        "                                                     n_neighbors=n_neighbors + 1)\n",
        "\n",
        "      # Exclude the first index (it will be the user itself)\n",
        "      self._similar_users = indices[0][1:]\n",
        "      return self._similar_users\n",
        "\n",
        "  @property\n",
        "  def similar_users(self) -> np.ndarray:\n",
        "    if self._similar_users is None:\n",
        "        return self.find_similar_users()\n",
        "    return self._similar_users"
      ],
      "metadata": {
        "id": "jSPUS1EHZU9k"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class provides user-based collaborative filtering recommendations. The class uses the similar users' interactions with items to recommend items to a given user, excluding items the user has already interacted with."
      ],
      "metadata": {
        "id": "VqDd8H6WoLLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IUserBasedRecommender(ABC):\n",
        "    @abstractmethod\n",
        "    def recommend(self, top_n: int = 5) -> List[int]:\n",
        "        \"\"\"Returns a list of item IDs recommended for the user.\"\"\"\n",
        "        pass\n",
        "\n",
        "class UserBasedRecommender(IUserBasedRecommender):\n",
        "    def __init__(self, user_id: int, user_item_matrix: pd.DataFrame,\n",
        "                 similar_user_finder: ISimilarUserFinder):\n",
        "        self.user_id = user_id\n",
        "        self.user_item_matrix = user_item_matrix\n",
        "        self.similar_user_finder = similar_user_finder\n",
        "\n",
        "    def recommend(self, top_n: int = 5) -> List[int]:\n",
        "        similar_user_indices = self.similar_user_finder.similar_users\n",
        "\n",
        "        similar_users_matrix = self.user_item_matrix.iloc[similar_user_indices]\n",
        "\n",
        "        item_scores = similar_users_matrix.sum(axis=0)\n",
        "\n",
        "        user_interactions = self.user_item_matrix.loc[self.user_id]\n",
        "\n",
        "        unseen_items = item_scores[user_interactions == 0]\n",
        "\n",
        "        top_items = unseen_items.sort_values(ascending=False).head(top_n)\n",
        "\n",
        "        return [(item, score) for item, score in top_items.items()]\n"
      ],
      "metadata": {
        "id": "ky4dRThSiKTP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Item based recommendation system  "
      ],
      "metadata": {
        "id": "aFyk6KHuFGZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IItemSimilarityFinder(ABC):\n",
        "    @abstractmethod\n",
        "    def find_similar_items(self, n_neighbors: int = 5) -> np.ndarray:\n",
        "      \"\"\"Finds and returns similar items to the given item ID.\"\"\"\n",
        "      pass\n",
        "\n",
        "class ItemSimilarityFinder(IItemSimilarityFinder):\n",
        "    def __init__(self, item_id: int, item_user_matrix: pd.DataFrame, knn_model: NearestNeighbors):\n",
        "        self.item_user_matrix = item_user_matrix\n",
        "        self.knn_model = knn_model\n",
        "        self.item_id = item_id\n",
        "        self._similar_videos = None\n",
        "\n",
        "    def find_similar_items(self, item_id: int, n_neighbors: int = 5):\n",
        "        if item_id not in self.item_user_matrix.index:\n",
        "            raise InvalidVideoIdError(item_id)\n",
        "\n",
        "        item_vector = self.item_user_matrix.loc[item_id].values.reshape(1, -1)\n",
        "        distances, indices = self.knn_model.kneighbors(item_vector, n_neighbors=n_neighbors + 1)\n",
        "\n",
        "        similar_items = self.item_user_matrix.index[indices[0][1:]].tolist()\n",
        "        similarities = distances[0][1:].tolist()\n",
        "\n",
        "        return similar_items, similarities\n",
        "\n",
        "    @property\n",
        "    def similar_videos(self) -> np.ndarray:\n",
        "        if self._similar_videos is None:\n",
        "            self._similar_videos = self.find_similar_items(self.item_id)\n",
        "        return self._similar_videos\n"
      ],
      "metadata": {
        "id": "7YHQSIA--uTf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IUserInteractionFetcher(ABC):\n",
        "    @abstractmethod\n",
        "    def get_interacted_items(self, user_id: int) -> List[int]:\n",
        "        pass\n",
        "\n",
        "class UserInteractionFetcher(IUserInteractionFetcher):\n",
        "    def __init__(self, user_item_matrix: pd.DataFrame, user_checker: IUserInteractionChecker):\n",
        "        self.user_item_matrix = user_item_matrix\n",
        "        self.user_checker = user_checker\n",
        "\n",
        "    def get_interacted_items(self, user_id: int) -> List[int]:\n",
        "        self.user_checker.check_user_interaction(user_id, self.user_item_matrix)\n",
        "\n",
        "        user_interactions = self.user_item_matrix.loc[user_id]\n",
        "        return user_interactions[user_interactions > 0].index.tolist()"
      ],
      "metadata": {
        "id": "wDxTaZGWFWsL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IItemScoreAccumulator(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def accumulate(self, similar_items: Iterable[int], interacted_items: Iterable[int]) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_scores(self) -> Dict[int, float]:\n",
        "        pass\n",
        "\n",
        "class ItemScoreAccumulator(IItemScoreAccumulator):\n",
        "    def __init__(self):\n",
        "        self._scores = {}\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self._scores.clear()\n",
        "\n",
        "\n",
        "    def accumulate(self, similar_items: Iterable[int], interacted_items: Iterable[int], similarities: Iterable[float]) -> None:\n",
        "        for item, similarity in zip(similar_items, similarities):\n",
        "            if item not in interacted_items:\n",
        "                self._scores[item] = self._scores.get(item, 0) + similarity\n",
        "\n",
        "    def get_scores(self) -> Dict[int, float]:\n",
        "        return self._scores\n"
      ],
      "metadata": {
        "id": "KCrLPCU7FYsl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IRecommendationRanker(ABC):\n",
        "    @abstractmethod\n",
        "    def rank(self, item_scores: Dict[int, float], top_n: int) -> List[int]:\n",
        "        pass\n",
        "\n",
        "class RecommendationRanker(IRecommendationRanker):\n",
        "    def rank(self, item_scores: Dict[int, float], top_n: int) -> List[int]:\n",
        "        sorted_items = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [item for item, _ in sorted_items[:top_n]]"
      ],
      "metadata": {
        "id": "u1xeBmHdFgKb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemBasedRecommender:\n",
        "    def __init__(self, user_id: int, user_item_matrix: pd.DataFrame,\n",
        "                 similarity_finder: IItemSimilarityFinder, score_accumulator: IItemScoreAccumulator,\n",
        "                 interaction_fetcher: IUserInteractionFetcher, ranker: IRecommendationRanker):\n",
        "        self.user_id = user_id\n",
        "        self.user_item_matrix = user_item_matrix\n",
        "        self.similarity_finder = similarity_finder\n",
        "        self.score_accumulator = score_accumulator\n",
        "        self.interaction_fetcher = interaction_fetcher\n",
        "        self.ranker = ranker\n",
        "\n",
        "    def recommend(self, top_n: int = 5) -> List[Tuple[int, float]]:\n",
        "        if self.user_id not in self.user_item_matrix.index:\n",
        "            raise ValueError(f\"User ID {self.user_id} not found in matrix.\")\n",
        "\n",
        "        interacted_items = self.interaction_fetcher.get_interacted_items(self.user_id)\n",
        "        self.score_accumulator.reset()\n",
        "\n",
        "        for item in interacted_items:\n",
        "            similar_items, similarities = self.similarity_finder.find_similar_items(item)\n",
        "            self.score_accumulator.accumulate(similar_items, interacted_items, similarities)\n",
        "\n",
        "        item_scores = self.score_accumulator.get_scores()\n",
        "        ranked_items = self.ranker.rank(item_scores, top_n)\n",
        "        recommended_items = [(item, item_scores.get(item, 0)) for item in ranked_items]\n",
        "        return recommended_items\n"
      ],
      "metadata": {
        "id": "BYasmfzC--b-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemSimilarityAdapter(IItemSimilarityFinder):\n",
        "    def __init__(self, item_user_matrix, knn_model):\n",
        "        self.item_user_matrix = item_user_matrix\n",
        "        self.knn_model = knn_model\n",
        "\n",
        "    def find_similar_items(self, item_id: int, n_neighbors: int = 5):\n",
        "        if item_id not in self.item_user_matrix.index:\n",
        "            raise InvalidVideoIdError(item_id)\n",
        "\n",
        "        item_vector = self.item_user_matrix.loc[item_id].values.reshape(1, -1)\n",
        "        distances, indices = self.knn_model.kneighbors(item_vector, n_neighbors=n_neighbors + 1)\n",
        "\n",
        "        similar_items = self.item_user_matrix.index[indices[0][1:]].tolist()\n",
        "        similarities = distances[0][1:].tolist()\n",
        "        return similar_items, similarities\n"
      ],
      "metadata": {
        "id": "eYxO0EjKXrgB"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filter"
      ],
      "metadata": {
        "id": "YTnTIu5U_5-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_collaborative_filter(Dataset_path: str, target_user_id: int,top_n:int):\n",
        "\n",
        "    df = DataPreparingManager(Load_and_shuffle_Dataset, Dataset_path, InteractionScorer)\n",
        "    df = df.prepare_data()\n",
        "\n",
        "    user_item_builder = UserItemMatrix(df, row=\"user_id\", column=\"video_id\", values=\"interaction_score\")\n",
        "    user_item_matrix = user_item_builder.matrix\n",
        "    item_user_matrix = user_item_matrix.T\n",
        "\n",
        "    user_sparse_converter = SparseMatrixConverter(user_item_matrix)\n",
        "    user_sparse_matrix = user_sparse_converter.sparse_matrix\n",
        "    item_sparse_converter = SparseMatrixConverter(item_user_matrix)\n",
        "    item_sparse_matrix = item_sparse_converter.sparse_matrix\n",
        "\n",
        "    user_knn_trainer = KNNModelTrainer(sparse_matrix=user_sparse_matrix, metric='cosine', algorithm='brute')\n",
        "    user_knn_model = user_knn_trainer.model\n",
        "    item_knn_trainer = KNNModelTrainer(sparse_matrix=item_sparse_matrix, metric='cosine', algorithm='brute')\n",
        "    item_knn_model = item_knn_trainer.model\n",
        "\n",
        "    user_checker = UserInteractionChecker()\n",
        "    user_index_finder = UserIndexFinder(user_id=target_user_id,\n",
        "                                        user_item_matrix=user_item_matrix,\n",
        "                                        user_check=user_checker)\n",
        "\n",
        "    similar_user_finder = SimilarUserFinder(user_id=target_user_id,\n",
        "                                            knn_model=user_knn_model,\n",
        "                                            user_item_matrix=user_item_matrix,\n",
        "                                            user_index=user_index_finder,\n",
        "                                            user_check=user_checker)\n",
        "\n",
        "    user_recommender = UserBasedRecommender(user_id=target_user_id,\n",
        "                                            user_item_matrix=user_item_matrix,\n",
        "                                            similar_user_finder=similar_user_finder)\n",
        "\n",
        "    user_recommendations = user_recommender.recommend(top_n)\n",
        "\n",
        "    user_recs_dict = {rec[0]: rec[1] for rec in user_recommendations}\n",
        "\n",
        "    interaction_checker = UserInteractionChecker()\n",
        "    interaction_fetcher = UserInteractionFetcher(user_item_matrix=user_item_matrix,\n",
        "                                                 user_checker=interaction_checker)\n",
        "    score_accumulator = ItemScoreAccumulator()\n",
        "    ranker = RecommendationRanker()\n",
        "\n",
        "    similarity_finder = ItemSimilarityAdapter(item_user_matrix, item_knn_model)\n",
        "\n",
        "    item_recommender = ItemBasedRecommender(user_id=target_user_id,\n",
        "                                            user_item_matrix=user_item_matrix,\n",
        "                                            similarity_finder=similarity_finder,\n",
        "                                            score_accumulator=score_accumulator,\n",
        "                                            interaction_fetcher=interaction_fetcher,\n",
        "                                            ranker=ranker)\n",
        "\n",
        "    item_recommendations = item_recommender.recommend(top_n)\n",
        "\n",
        "    item_recs_dict = {rec[0]: rec[1] for rec in item_recommendations}\n",
        "\n",
        "    return user_recs_dict, item_recs_dict\n"
      ],
      "metadata": {
        "id": "DEO3RTIPXww0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Content Filter**"
      ],
      "metadata": {
        "id": "mbCxp82kpJDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFramePreprocessor:\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df\n",
        "\n",
        "    def drop_columns(self, columns: List[str]):\n",
        "        self.df = self.df.drop(columns=columns, axis=1)\n",
        "        return self\n",
        "\n",
        "    def remove_duplicates(self, subset: List[str] = [\"video_id\"]):\n",
        "        self.df = self.df.drop_duplicates(subset=subset)\n",
        "        return self\n",
        "\n",
        "    def get(self):\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "P1eQrNV6va7r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor(ABC):\n",
        "  @abstractmethod\n",
        "  def _clean_text(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def _tokenize(self):\n",
        "      pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def text_processor(self, text):\n",
        "      pass\n",
        "\n",
        "class EnglishTextPreprocessor(TextPreprocessor):\n",
        "  def __init__(self):\n",
        "      self.stopwords = set(stopwords.words('english'))\n",
        "      self.stemmer = SnowballStemmer('english')\n",
        "      self.tokens = []\n",
        "      self.text = \"\"\n",
        "\n",
        "  def _clean_text(self):\n",
        "      \"\"\"Converts text to lowercase and tokenizes it.\"\"\"\n",
        "      self.tokens = word_tokenize(self.text.lower())\n",
        "\n",
        "  def _tokenize(self):\n",
        "      \"\"\"Removes stopwords and punctuation and applies stemming.\"\"\"\n",
        "      self.tokens = [\n",
        "          self.stemmer.stem(word)\n",
        "          for word in self.tokens\n",
        "          if word not in self.stopwords and word not in string.punctuation\n",
        "      ]\n",
        "      return self.tokens\n",
        "\n",
        "  def text_processor(self, text):\n",
        "      \"\"\"Process the text: clean and tokenize.\"\"\"\n",
        "      self.text = text\n",
        "      self._clean_text()\n",
        "      return self._tokenize()\n",
        "\n",
        "class ArabicTextPreprocessor(TextPreprocessor):\n",
        "  def __init__(self):\n",
        "      self.stopwords = set(stopwords.words('arabic'))\n",
        "      self.stemmer = CamelStemmer()\n",
        "      self.tokens = []\n",
        "      self.text = \"\"\n",
        "\n",
        "  def _clean_text(self):\n",
        "      \"\"\"Tokenizes Arabic text.\"\"\"\n",
        "      self.tokens = arabic_tokenizer(self.text)\n",
        "\n",
        "  def _tokenize(self):\n",
        "      \"\"\"Removes stopwords and punctuation and applies stemming.\"\"\"\n",
        "      self.tokens = [\n",
        "          self.stemmer.stem(word)\n",
        "          for word in self.tokens\n",
        "          if word not in self.stopwords and word not in string.punctuation\n",
        "      ]\n",
        "      return self.tokens\n",
        "\n",
        "  def text_processor(self, text):\n",
        "      \"\"\"Process the text: clean and tokenize.\"\"\"\n",
        "      self.text = text\n",
        "      self._clean_text()\n",
        "      return self._tokenize()"
      ],
      "metadata": {
        "id": "_VN6bm96-fD6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataFrameTextProcessor:\n",
        "    def __init__(self, preprocessor: TextPreprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def process(self, df: pd.DataFrame, title_col: str, genre_col: str, new_col_name: str = \"tokenized_text\") -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        df[\"combined_text\"] = df[title_col] + \" \" + df[genre_col]\n",
        "        df[new_col_name] = df[\"combined_text\"].apply(self.preprocessor.text_processor)\n",
        "        return df"
      ],
      "metadata": {
        "id": "LmXGqnPk3DUy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecManager:\n",
        "\n",
        "    @staticmethod\n",
        "    def download_pretrained(name: str = \"word2vec-google-news-300\"):\n",
        "        return api.load(name)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(model, path: str):\n",
        "        model.save(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_model(path: str):\n",
        "        return KeyedVectors.load(path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_gensim_binary(path: str):\n",
        "        return KeyedVectors.load_word2vec_format(path, binary=True)\n"
      ],
      "metadata": {
        "id": "dKX9MSD91KMp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(corpus):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    return tfidf_matrix, vectorizer.vocabulary_"
      ],
      "metadata": {
        "id": "BhQrkbTHX2Nb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_corpus_from_tokens(token_column):\n",
        "    return token_column.apply(lambda tokens: \" \".join(tokens))"
      ],
      "metadata": {
        "id": "6OkA_2FuX4IL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_word2vec(tokens, index, model, tfidf_matrix, tfidf_vocab, vector_size=300):\n",
        "    vector = np.zeros(vector_size)\n",
        "    weight_sum = 0\n",
        "\n",
        "    for word in tokens:\n",
        "        if word not in model or word not in tfidf_vocab:\n",
        "            continue\n",
        "        weight = tfidf_matrix[index, tfidf_vocab[word]]\n",
        "        vector += model[word] * weight\n",
        "        weight_sum += weight\n",
        "\n",
        "    return vector / weight_sum if weight_sum > 0 else vector\n"
      ],
      "metadata": {
        "id": "xdKTJTX_X6gJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_video_vectors(df, model, tfidf_matrix, tfidf_vocab):\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"video_vector\"] = df.apply(\n",
        "        lambda row: get_weighted_word2vec(\n",
        "            row[\"tokenized_text\"],\n",
        "            row.name,\n",
        "            model,\n",
        "            tfidf_matrix,\n",
        "            tfidf_vocab\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    return df"
      ],
      "metadata": {
        "id": "gA6dOtIGX7HK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(vectors):\n",
        "    return cosine_similarity(np.vstack(vectors))"
      ],
      "metadata": {
        "id": "0UmforWLX83t"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_similar_videos(video_indices, df, similarity_matrix, top_n=5):\n",
        "    recommendations = []\n",
        "    for video_index in video_indices:\n",
        "        similarities = similarity_matrix[video_index]\n",
        "        similar_indices = np.argsort(similarities)[::-1]\n",
        "        top_similar = [i for i in similar_indices if i != video_index and i not in video_indices][:top_n]\n",
        "\n",
        "        recommendations.extend([\n",
        "            {\n",
        "                \"title\": df.iloc[i][\"video_title\"],\n",
        "                \"genre\": df.iloc[i][\"video_genre\"],\n",
        "                \"score\": round(float(similarities[i]), 4)\n",
        "            }\n",
        "            for i in top_similar\n",
        "        ])\n",
        "\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "AeGZeSfnX-gV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content_based_recommendations(csv_path: str, watched_video_indices: List[int], top_n: int ):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df_preprocessed = (\n",
        "        DataFramePreprocessor(df)\n",
        "        .drop_columns(['user_id', 'watched', 'liked', 'commented', 'subscribed'])\n",
        "        .remove_duplicates(subset=[\"video_title\", \"video_genre\"])\n",
        "        .get()\n",
        "    )\n",
        "\n",
        "    text_preprocessor = EnglishTextPreprocessor()\n",
        "    text_processor = DataFrameTextProcessor(text_preprocessor)\n",
        "    df_text_processed = text_processor.process(df_preprocessed,\n",
        "                                               title_col=\"video_title\",\n",
        "                                               genre_col=\"video_genre\"\n",
        "                                               )\n",
        "\n",
        "    corpus = build_corpus_from_tokens(df_text_processed[\"tokenized_text\"])\n",
        "    tfidf_matrix, tfidf_vocab = compute_tfidf(corpus)\n",
        "\n",
        "    word2vec = Word2VecManager()\n",
        "    if not os.path.exists(\"word2vec_model.bin\"):\n",
        "        model = word2vec.download_pretrained()\n",
        "        word2vec.save_model(model, \"word2vec_model.bin\")\n",
        "    else:\n",
        "        model = word2vec.load_model(\"word2vec_model.bin\")\n",
        "\n",
        "    df_vectors = compute_video_vectors(df_text_processed, model, tfidf_matrix, tfidf_vocab)\n",
        "    similarity_matrix = compute_cosine_similarity(df_vectors[\"video_vector\"].tolist())\n",
        "\n",
        "    agg_similarity = np.zeros(similarity_matrix.shape[0])\n",
        "    for idx in watched_video_indices:\n",
        "        agg_similarity += similarity_matrix[idx]\n",
        "\n",
        "    for idx in watched_video_indices:\n",
        "        agg_similarity[idx] = -1\n",
        "\n",
        "    recommended_indices = np.argsort(agg_similarity)[::-1][:top_n]\n",
        "\n",
        "    recommendations = [\n",
        "        {\n",
        "            \"title\": df_vectors.iloc[i][\"video_title\"],\n",
        "            \"genre\": df_vectors.iloc[i][\"video_genre\"],\n",
        "            \"score\": round(float(agg_similarity[i]), 4)\n",
        "        }\n",
        "        for i in recommended_indices\n",
        "    ]\n",
        "\n",
        "    return recommendations\n"
      ],
      "metadata": {
        "id": "dFmv3LS5-BKF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_hybrid_recommendations(data_set_path: str,\n",
        "                                user_id: int,\n",
        "                                user_weight=0.4,\n",
        "                                item_weight=0.4,\n",
        "                                content_weight=0.2,\n",
        "                                k=10,\n",
        "                                n_recommendations=10):\n",
        "\n",
        "    df = pd.read_csv(data_set_path)\n",
        "    df = df[pd.to_numeric(df['video_id'], errors='coerce').notna()]\n",
        "    df['video_id'] = df['video_id'].astype(int)\n",
        "\n",
        "    user_recs, item_recs = get_collaborative_filter(data_set_path, user_id,top_n = k)\n",
        "\n",
        "    final_scores = {}\n",
        "    for vid, score in user_recs.items():\n",
        "        final_scores[vid] = final_scores.get(vid, 0) + score * user_weight\n",
        "\n",
        "    for vid, score in item_recs.items():\n",
        "        final_scores[vid] = final_scores.get(vid, 0) + score * item_weight\n",
        "\n",
        "    watched_video_ids = df[df['user_id'] == user_id]['video_id'].tolist()\n",
        "\n",
        "    valid_watched_video_ids = [video_id for video_id in watched_video_ids if video_id in df['video_id'].values]\n",
        "\n",
        "    valid_watched_video_indices = [df[df['video_id'] == video_id].index[0] for video_id in valid_watched_video_ids]\n",
        "\n",
        "    content_recs = get_content_based_recommendations(csv_path=data_set_path,\n",
        "                                                     watched_video_indices=valid_watched_video_indices,\n",
        "                                                     top_n=k)\n",
        "\n",
        "    for rec in content_recs:\n",
        "        video_title = rec[\"title\"]\n",
        "        video_id = df[df['video_title'] == video_title]['video_id'].values[0]\n",
        "        score = rec[\"score\"]\n",
        "        final_scores[video_id] = final_scores.get(video_id, 0) + score * content_weight\n",
        "\n",
        "    sorted_recs = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    top_recs = sorted_recs[:n_recommendations]\n",
        "\n",
        "    final_recommendations = []\n",
        "    for video_id, score in top_recs:\n",
        "        video_id = int(video_id)\n",
        "        title = df[df['video_id'] == video_id]['video_title'].values[0] if len(df[df['video_id'] == video_id]) > 0 else \"Unknown\"\n",
        "        final_recommendations.append({\n",
        "            \"video_id\": video_id,\n",
        "            \"title\": title\n",
        "        })\n",
        "\n",
        "    output_filename = f\"hybrid_recommendations_user_{user_id}.json\"\n",
        "    with open(output_filename, \"w\", encoding='utf-8') as f:\n",
        "        json.dump(final_recommendations, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nHybrid recommendations saved to '{output_filename}'.\")\n",
        "\n",
        "def main():\n",
        "    Dataset_path = \"/content/modified_video_recommendation_dataset.csv\"\n",
        "    hybrid_recommendations_file = get_hybrid_recommendations(Dataset_path,\n",
        "                                                             user_id=10,\n",
        "                                                             user_weight=0.4,\n",
        "                                                             item_weight=0.4,\n",
        "                                                             content_weight=0.2,\n",
        "                                                             k=20,\n",
        "                                                             n_recommendations=20)\n",
        "    print(f\"Recommendations written to: {hybrid_recommendations_file}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJkmmGe32TwB",
        "outputId": "df870f1b-566f-48d4-a3e7-ea1d033eeac9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hybrid recommendations saved to 'hybrid_recommendations_user_10.json'.\n",
            "Recommendations written to: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation System Error Exceptions"
      ],
      "metadata": {
        "id": "Lu9nXysud7QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommendationSystemError(Exception):\n",
        "    \"\"\"Base class for all exceptions in the recommendation system.\"\"\"\n",
        "    pass\n",
        "\n",
        "class InvalidMatrixError(RecommendationSystemError):\n",
        "    \"\"\"Raised when a matrix is invalid or does not meet the required conditions.\"\"\"\n",
        "\n",
        "    def __init__(self, message=\"Matrix is invalid\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class InvalidUserIdError(RecommendationSystemError):\n",
        "    \"\"\"Raised when a user ID is not found in the system.\"\"\"\n",
        "\n",
        "    def __init__(self, user_id, message=\"User ID not found\"):\n",
        "        self.user_id = user_id\n",
        "        self.message = f\"{message}: {user_id}\"\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class TrainingError(RecommendationSystemError):\n",
        "    \"\"\"Raised when an error occurs during model training.\"\"\"\n",
        "\n",
        "    def __init__(self, message=\"Model training failed\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class InvalidVideoIdError(RecommendationSystemError):\n",
        "  \"\"\"Raised when a video ID is not found in the system.\"\"\"\n",
        "\n",
        "  def __init__(self, video_id, message=\"Video ID not found\"):\n",
        "      self.video_id = video_id\n",
        "      self.message = f\"{message}: {video_id}\"\n",
        "      super().__init__(self.message)\n",
        "\n",
        "class MissingRequiredData(RecommendationSystemError):\n",
        "    \"\"\"Raised when required data is missing.\"\"\"\n",
        "\n",
        "    def __init__(self, message=\"Missing required data\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)\n",
        "\n",
        "class InvalidInputError(RecommendationSystemError):\n",
        "    \"\"\"Raised when invalid input is provided.\"\"\"\n",
        "\n",
        "    def __init__(self, message=\"Invalid input\"):\n",
        "        self.message = message\n",
        "        super().__init__(self.message)"
      ],
      "metadata": {
        "id": "6auDiQcsdsxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}